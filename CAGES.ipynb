{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYLFpvoEzPHT",
        "outputId": "4227f79a-3543-4845-d00e-3f1639501043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting botorch\n",
            "  Downloading botorch-0.11.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from botorch) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from botorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from botorch) (1.3.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from botorch) (2.4.0+cu121)\n",
            "Collecting pyro-ppl>=1.8.4 (from botorch)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting gpytorch==1.12 (from botorch)\n",
            "  Downloading gpytorch-1.12-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting linear-operator==0.5.2 (from botorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from botorch) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch==1.12->botorch) (1.3.2)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator==0.5.2->botorch)\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator==0.5.2->botorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.8.4->botorch)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8.4->botorch) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->botorch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.1->botorch) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch==1.12->botorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch==1.12->botorch) (3.5.0)\n",
            "Downloading botorch-0.11.3-py3-none-any.whl (631 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.9/631.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gpytorch-1.12-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.1/274.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pyro-api, typeguard, jaxtyping, pyro-ppl, linear-operator, gpytorch, botorch\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed botorch-0.11.3 gpytorch-1.12 jaxtyping-0.2.34 linear-operator-0.5.2 pyro-api-0.1.2 pyro-ppl-1.9.1 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install botorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyDOE2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeI1PXI0zj1N",
        "outputId": "786af67e-979a-4f0b-def6-da03b35797bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyDOE2\n",
            "  Downloading pyDOE2-1.3.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE2) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE2) (1.13.1)\n",
            "Building wheels for collected packages: pyDOE2\n",
            "  Building wheel for pyDOE2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE2: filename=pyDOE2-1.3.0-py3-none-any.whl size=25523 sha256=56544bd9fbfae7509afc521f9660d7245376b49bdfb052f0d0561abfcfdbaa32\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/1f/29/6fda5c72f950841e39147ae603780ee913a62f977b4ad47ee4\n",
            "Successfully built pyDOE2\n",
            "Installing collected packages: pyDOE2\n",
            "Successfully installed pyDOE2-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from botorch import fit_gpytorch_mll\n",
        "from botorch.models.cost import AffineFidelityCostModel\n",
        "from botorch.acquisition.cost_aware import InverseCostWeightedUtility\n",
        "from botorch.acquisition import PosteriorMean\n",
        "from botorch.acquisition.knowledge_gradient import qMultiFidelityKnowledgeGradient\n",
        "from botorch.acquisition.fixed_feature import FixedFeatureAcquisitionFunction\n",
        "from botorch.optim.optimize import optimize_acqf\n",
        "from botorch.acquisition.utils import project_to_target_fidelity\n",
        "from botorch.models.gp_regression_fidelity import SingleTaskMultiFidelityGP\n",
        "from botorch.models.gp_regression_mixed import MixedSingleTaskGP\n",
        "from botorch.models.transforms.outcome import Standardize\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.optim.optimize import optimize_acqf_mixed\n",
        "from torch import Tensor\n",
        "from botorch.models.deterministic import DeterministicModel\n",
        "import numpy as np\n",
        "from gpytorch.kernels import MaternKernel, RBFKernel, ScaleKernel\n",
        "from typing import Any, Callable, Dict, List, Optional\n",
        "from gpytorch.constraints import GreaterThan\n",
        "from botorch.acquisition import AnalyticAcquisitionFunction\n",
        "import gpytorch\n",
        "from pyDOE2 import lhs"
      ],
      "metadata": {
        "id": "lIaS0wIYzSSF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkwargs = {\n",
        "    \"dtype\": torch.double,\n",
        "    # \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"device\": torch.device(\"cpu\"),\n",
        "}\n",
        "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "3ULLfQQv0SyH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define test function"
      ],
      "metadata": {
        "id": "JmTZuTEk4acv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rosenbrock():\n",
        "    def __init__(self,dim, LVGP=True, negate=True):\n",
        "        self.dim=dim\n",
        "        self.LVGP = LVGP\n",
        "        self.negate = negate\n",
        "    def __call__(self,X):\n",
        "        X = X.clone()\n",
        "        X[X[:, -1] == 2, -1] = 0.9\n",
        "        fun_val = 0\n",
        "        for d in range(1,self.dim):\n",
        "            X_curr = X[..., d-1:d]\n",
        "            X_next = X[..., d:d+1]\n",
        "\n",
        "            if self.LVGP:\n",
        "                t1 = 100 * (X_next - X_curr**2) ** 2 + (1-X[..., -1:])*torch.sin(10*X_curr+5*X_next) # refer to MISO paper\n",
        "            else:\n",
        "                t1 = 100 * (X_next - X_curr**2) ** 2\n",
        "            t2 = (X_curr - 1) ** 2\n",
        "            fun_val+=((t1 + t2).sum(dim=-1))\n",
        "        if self.negate:\n",
        "            fun_val = fun_val*-1\n",
        "        return fun_val"
      ],
      "metadata": {
        "id": "qz3OCqrmzn-C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define cost function"
      ],
      "metadata": {
        "id": "jan6Lnf-4gE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlexibleFidelityCostModel(DeterministicModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        fidelity_dims: list = [-1],\n",
        "         values = {'2': 1, '1.0': 10},\n",
        "         fixed_cost: float = 0,\n",
        "         )->None:\n",
        "        r'Gets the cost according to the fidelity level'\n",
        "        super().__init__()\n",
        "        self.cost_values=values\n",
        "        self.fixed_cost=fixed_cost\n",
        "        self.fidelity_dims=fidelity_dims\n",
        "        self.register_buffer(\"weights\", torch.tensor([1.0]))\n",
        "        self._num_outputs = 1\n",
        "\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "\n",
        "        cost = list(map(lambda x: self.cost_values[str(float(x))], X[..., self.fidelity_dims].flatten()))\n",
        "        cost = torch.tensor(cost).to(X)\n",
        "        cost = cost.reshape(X[..., self.fidelity_dims].shape)\n",
        "        return self.fixed_cost + cost"
      ],
      "metadata": {
        "id": "K-u-VyzWzz3e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define custom acquisition function (CAGES)"
      ],
      "metadata": {
        "id": "OBi29BZD4j5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAcquisitionFunction(AnalyticAcquisitionFunction):\n",
        "    def __init__(self, model, current_theta):\n",
        "        '''Inits acquisition function with model.'''\n",
        "        super().__init__(model=model)\n",
        "        self.current_theta = current_theta # point that we want to evaluate gradient\n",
        "\n",
        "    def _get_KxX_dx(self, x, X) -> torch.Tensor:\n",
        "        '''Computes the analytic derivative of the kernel K((x1,c1),(x2,c2)) w.r.t. x1.\n",
        "        Noted that we care only about the gradient at the highest fidelity function'''\n",
        "\n",
        "        x_cont = x[:,0:-1]\n",
        "        X_cont = X[:,0:-1]\n",
        "        N = X.shape[0]\n",
        "        n = x.shape[0]\n",
        "        self.model.D = X.shape[1] - 1 # dimension for continuous variable\n",
        "\n",
        "        outputscale1 = self.model.covar_module.kernels[0].outputscale\n",
        "        outputscale2 = self.model.covar_module.kernels[1].outputscale\n",
        "        K_xX_SE1 = outputscale1*self.model.covar_module.kernels[0].base_kernel.kernels[0](x_cont, X_cont).evaluate() # covariance vector for SE kernel\n",
        "        K_xX_SE2 = outputscale2*self.model.covar_module.kernels[1].base_kernel.kernels[0](x_cont, X_cont).evaluate()\n",
        "\n",
        "        lengthscale1 = self.model.covar_module.kernels[0].base_kernel.kernels[0].lengthscale.detach() # lengthscale for the RBF kernel\n",
        "        lengthscale2 = self.model.covar_module.kernels[1].base_kernel.kernels[0].lengthscale.detach() # lengthscale for the RBF kernel\n",
        "        K_cat = self.model.covar_module.kernels[1].base_kernel.kernels[1](x, X).evaluate() # covariance vector for categorical kernel\n",
        "\n",
        "        dk_SE1_dx = -torch.eye(self.model.D, device=X.device)/ lengthscale1 ** 2 @ ((x_cont.view(n, 1, self.model.D) - X_cont.view(1, N, self.model.D))* K_xX_SE1.view(n, N, 1)).transpose(1, 2) # gradient of SE kernel\n",
        "        dk_SE2_dx = -torch.eye(self.model.D, device=X.device)/ lengthscale2 ** 2 @ ((x_cont.view(n, 1, self.model.D) - X_cont.view(1, N, self.model.D))* K_xX_SE2.view(n, N, 1)).transpose(1, 2)\n",
        "        return dk_SE1_dx + dk_SE2_dx * K_cat\n",
        "\n",
        "    def _get_KxX_dx2(self, x, X):\n",
        "        '''Computes the analytic second derivative of the kernel K((x1,c1),(x2,c2)) w.r.t. x1.'''\n",
        "        outputscale1 = self.model.covar_module.kernels[0].outputscale\n",
        "        outputscale2 = self.model.covar_module.kernels[1].outputscale\n",
        "        lengthscale1 = self.model.covar_module.kernels[0].base_kernel.kernels[0].lengthscale.detach() # lengthscale for the RBF kernel\n",
        "        lengthscale2 = self.model.covar_module.kernels[1].base_kernel.kernels[0].lengthscale.detach() # lengthscale for the RBF kernel\n",
        "        K_cat = self.model.covar_module.kernels[1].base_kernel.kernels[1](x, X).evaluate() # covariance vector for categorical kernel\n",
        "\n",
        "        dk_SE1_dx2 = outputscale1*torch.eye(self.model.D, device=X.device)/ lengthscale1 ** 2\n",
        "        dk_SE2_dx2 = outputscale2*torch.eye(self.model.D, device=X.device)/ lengthscale2 ** 2\n",
        "\n",
        "        return dk_SE1_dx2 + dk_SE2_dx2*K_cat\n",
        "\n",
        "    def calculate_gradient(self):\n",
        "        \"\"\"Compute the gradient for posterior mean function\"\"\"\n",
        "        X = self.model.train_inputs[0] # training data for GP\n",
        "        Y = self.model.train_targets\n",
        "        noise = self.model.likelihood.noise\n",
        "        KxX_dx = self._get_KxX_dx(self.current_theta, X)\n",
        "        K_XX = self.model.covar_module(X, X).evaluate()\n",
        "        K_XX_inv = torch.inverse(K_XX + noise*torch.eye(len(X), device=X.device))\n",
        "        prior_mean = self.model.mean_module(X)[0]\n",
        "        mean = KxX_dx @ K_XX_inv @ (Y - prior_mean) # graident of posterior mean\n",
        "        return mean\n",
        "\n",
        "    # @t_batch_mode_transform(expected_q=1)\n",
        "    def forward(self,thetas):\n",
        "        \"\"\"Compute the acquisition function value at thetas.\"\"\"\n",
        "\n",
        "        acquisition_values = []\n",
        "        for theta_aug in thetas:\n",
        "            noise = self.model.likelihood.noise\n",
        "\n",
        "            # differential entropy of gradient before augmenting imaginary data point\n",
        "            X_old = self.model.train_inputs[0]\n",
        "            KxX_dx_old = self._get_KxX_dx(self.current_theta, X_old)\n",
        "            K_XX_old = self.model.covar_module(X_old, X_old).evaluate()\n",
        "            K_XX_inv_old = torch.inverse(K_XX_old + noise*torch.eye(len(X_old), device=X_old.device))\n",
        "            K_xX_dx2_old = self._get_KxX_dx2(self.current_theta, self.current_theta)\n",
        "            variance_old = K_xX_dx2_old - KxX_dx_old @ K_XX_inv_old @ KxX_dx_old.transpose(1,2) # variance of gradient\n",
        "            log_det_old = torch.logdet(variance_old)\n",
        "\n",
        "            # differential entropy of gradient after augmenting imaginary data point (theta_aug)\n",
        "            X_aug = torch.cat((self.model.train_inputs[0], theta_aug))\n",
        "            KxX_dx = self._get_KxX_dx(self.current_theta, X_aug)\n",
        "            K_XX = self.model.covar_module(X_aug, X_aug).evaluate()\n",
        "            K_XX_inv = torch.inverse(K_XX + noise*torch.eye(len(X_aug), device=X_aug.device))\n",
        "            K_xX_dx2 = self._get_KxX_dx2(self.current_theta, self.current_theta)\n",
        "            variance_new = K_xX_dx2 - KxX_dx @ K_XX_inv @ KxX_dx.transpose(1,2) # variance of gradient\n",
        "            log_det_new  = torch.logdet(variance_new)\n",
        "\n",
        "            acq_val = 0.5*(log_det_old - log_det_new)/float(cost_model(theta_aug)[0][0])\n",
        "            acquisition_values.append(acq_val)\n",
        "\n",
        "        return torch.cat(acquisition_values,dim=0).flatten()"
      ],
      "metadata": {
        "id": "Azquik5mz-B9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define kernel for MixedSingleTask GP"
      ],
      "metadata": {
        "id": "13YhQqY14qvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cont_kernel_factory(\n",
        "    batch_shape: torch.Size,\n",
        "    ard_num_dims: int,\n",
        "    active_dims: List[int],\n",
        ") -> RBFKernel:\n",
        "    return RBFKernel(\n",
        "        batch_shape=batch_shape,\n",
        "        ard_num_dims=ard_num_dims,\n",
        "        active_dims=active_dims,\n",
        "        lengthscale_constraint=GreaterThan(1e-04),\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "fVM2Ioab0AGm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for generating initial training data for GP"
      ],
      "metadata": {
        "id": "x0UsGSng4uri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_data(seed = None, fun = None, dim=-1):\n",
        "\n",
        "    np.random.seed((seed)*1)\n",
        "    N_l1 = 4 # number of training data for the first level\n",
        "    N_l2 = 5 # number of training data for the second level\n",
        "    ind_qual = [dim] # column index for the qualatative variable\n",
        "\n",
        "    X_te_normalized = 0.2+0.6*np.random.rand(1,dim) # random select a point as the starting location for CAGES\n",
        "    X_te = lb+(ub-lb)*X_te_normalized  # rescale\n",
        "    qualatative_column_te = np.random.choice([1], size=1) # we want to estimate the gradient for the highest fidelity function\n",
        "    if ind_qual is not None:\n",
        "        X_te = np.column_stack((X_te, qualatative_column_te)) # concatenate the qualatative variable into testing data\n",
        "        X_te_normalized  = np.column_stack((X_te_normalized , qualatative_column_te))\n",
        "\n",
        "    # generate initial training data (at level1) for GP\n",
        "    X_l1_normalized = lhs(dim, samples = N_l1, random_state=seed)\n",
        "    X_l1 = lb+(ub-lb)*X_l1_normalized\n",
        "    qualatative_column = np.random.choice([1.0], size=N_l1)\n",
        "    if ind_qual is not None:\n",
        "        X_l1 = np.column_stack((X_l1, qualatative_column)) # concatenate the qualatative varibale into training data set\n",
        "        X_l1_normalized = np.column_stack((X_l1_normalized, qualatative_column))\n",
        "\n",
        "    # generate initial training data (at level2) for GP\n",
        "    X_l2_normalized = lhs(dim, samples = N_l2, random_state=seed+10)\n",
        "    X_l2 = lb+(ub-lb)*X_l2_normalized\n",
        "    qualatative_column = np.random.choice([2.0], size=N_l2)\n",
        "    if ind_qual is not None:\n",
        "        X_l2 = np.column_stack((X_l2, qualatative_column)) # concatenate the qualatative varibale into training data set\n",
        "        X_l2_normalized = np.column_stack((X_l2_normalized, qualatative_column))\n",
        "\n",
        "    train_x_full= np.concatenate((X_l1, X_l2))\n",
        "    train_x_full_normalized = np.concatenate((X_l1_normalized, X_l2_normalized))\n",
        "    train_x_full = np.concatenate((train_x_full, X_te)) # need to include the estimated point into the training data\n",
        "    train_x_full_normalized = np.concatenate((train_x_full_normalized, X_te_normalized))\n",
        "\n",
        "    train_obj = fun(torch.tensor(train_x_full)).unsqueeze(1) # calculate the true function value\n",
        "    return torch.tensor(train_x_full_normalized), train_obj"
      ],
      "metadata": {
        "id": "r5IZRDuy0Cda"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(train_x, train_obj, dim):\n",
        "    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint = GreaterThan(1e-04))\n",
        "    model = MixedSingleTaskGP(train_x, train_obj, outcome_transform=Standardize(m=1), likelihood = likelihood, cat_dims = [dim], cont_kernel_factory=cont_kernel_factory)\n",
        "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
        "    return mll, model"
      ],
      "metadata": {
        "id": "HCuTNMmR0Esd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    dim = 12\n",
        "    dim_all = dim+1 # dimension including categorical variable\n",
        "    lr = 0.05 # learning rate for gradient ascent\n",
        "    N_ITER = 4 # number of outer BO iteration\n",
        "    N_iner = int(0.5*dim) # number of inner iteration to decrease gradient uncertainty\n",
        "    replicate = 1 # number of experiment replicate\n",
        "    initial_cost = 55\n",
        "    cost_budget = 400\n",
        "    lb = np.array([0]*(dim)) # lb for Rosenbrock\n",
        "    ub = np.array([2]*(dim)) # ub for Rosenbrock\n",
        "    bounds = torch.tensor([[0.0] * dim, [1.0] * dim], **tkwargs) # bounds for optimizing acquisition\n",
        "    bound_cat = torch.tensor([1.0,2.0]) # bound for categorical variable\n",
        "    bounds = torch.cat((bounds, bound_cat.unsqueeze(1)), dim=1)\n",
        "\n",
        "    # Define testing function\n",
        "    fun = Rosenbrock(dim=dim, LVGP=True, negate=True)\n",
        "    cost_model = FlexibleFidelityCostModel(values={'1.0':10, '2.0':1}, fixed_cost=0)\n",
        "    cost_aware_utility = InverseCostWeightedUtility(cost_model=cost_model)\n",
        "    cost_list = [[] for _ in range(replicate)]\n",
        "    best_Y_list = [[] for _ in range((replicate))]\n",
        "\n",
        "    for seed in range(replicate):\n",
        "        train_x, train_obj = generate_initial_data(seed=seed, fun = fun, dim=dim) # generate initial training data for GP\n",
        "        train_x = train_x.to(device)\n",
        "        train_obj = train_obj.to(device)\n",
        "        cost_list[seed].append(initial_cost)\n",
        "        best_Y_list[seed].append(-float(train_obj[-1]))\n",
        "        cumulative_cost = initial_cost\n",
        "\n",
        "        theta = train_x[-1].unsqueeze(0).clone() # starting point of gradient ascent\n",
        "\n",
        "        # Outer BO loop\n",
        "        for i in range(N_ITER):\n",
        "            mll, model = initialize_model(train_x, train_obj, dim)\n",
        "            try:\n",
        "                fit_gpytorch_mll(mll)\n",
        "            except:\n",
        "                print('cant fit GP')\n",
        "\n",
        "            CAGES_acq = CustomAcquisitionFunction(model, theta) # initialize acquisition function\n",
        "\n",
        "            # inner loop for querying point to decrease gradient uncertainty\n",
        "            for I in range(N_iner):\n",
        "                lb_acq = theta.clone()\n",
        "                ub_acq = theta.clone()\n",
        "                lb_acq = lb_acq - 0.1 # define boundary (centered around current theta)\n",
        "                ub_acq = ub_acq + 0.1\n",
        "                bounds_acq = torch.cat([lb_acq, ub_acq])\n",
        "                bounds_acq[bounds_acq>1] = 1\n",
        "                bounds_acq[bounds_acq<0] = 0\n",
        "\n",
        "                candidate, _ = optimize_acqf_mixed(\n",
        "                    acq_function=CAGES_acq,\n",
        "                    bounds = bounds_acq,\n",
        "                    fixed_features_list=[{dim: 2.0}, {dim: 1.0}],\n",
        "                    q=1,\n",
        "                    num_restarts=5,\n",
        "                    raw_samples=20\n",
        "                )\n",
        "\n",
        "                new_x = candidate.clone()\n",
        "                new_x[:,0:dim] = torch.tensor(lb).to(device) + (torch.tensor(ub-lb)).to(device)*new_x[:,0:dim]\n",
        "                new_obj = fun(new_x).unsqueeze(1)\n",
        "                train_x = torch.cat([train_x, candidate]).detach()\n",
        "                train_obj = torch.cat([train_obj, new_obj]).detach()\n",
        "\n",
        "                mll, model = initialize_model(train_x, train_obj, dim)\n",
        "                try:\n",
        "                    fit_gpytorch_mll(mll)\n",
        "                except:\n",
        "                    print('cant fit GP')\n",
        "\n",
        "                CAGES_acq = CustomAcquisitionFunction(model, theta)\n",
        "                cost = float(cost_model(new_x)[0][0])\n",
        "                cumulative_cost += cost\n",
        "\n",
        "\n",
        "            # moving via gradient ascent\n",
        "            gradient = CAGES_acq.calculate_gradient() # posterior mean gradient\n",
        "            theta[:,0:dim] = (theta[:,0:dim] + lr*gradient/torch.norm(gradient)).clone()\n",
        "\n",
        "            theta_rescale = theta.clone()\n",
        "            theta_rescale[:,0:dim] = torch.tensor(lb) + (torch.tensor(ub-lb))*theta_rescale[:,0:dim]\n",
        "            new_obj = fun(theta_rescale.clone()).unsqueeze(1)\n",
        "            train_x = torch.cat([train_x, theta]).detach()\n",
        "            train_obj = torch.cat([train_obj, new_obj]).detach()\n",
        "            cost = float(cost_model(theta)[0][0])\n",
        "            cumulative_cost += cost\n",
        "            cost_list[seed].append(int(cumulative_cost))\n",
        "            best_Y_list[seed].append(min(best_Y_list[seed][-1],-float(new_obj[0])))\n",
        "            print('Current function value = ',new_obj)\n",
        "            if cumulative_cost>cost_budget:\n",
        "                break\n",
        "\n",
        "    max_length = max(len(row) for row in cost_list)\n",
        "    padded_list = np.array([row + [row[-1]] * (max_length - len(row)) for row in cost_list])\n",
        "    xx = torch.tensor(padded_list)\n",
        "\n",
        "    max_length1 = max(len(row) for row in best_Y_list)\n",
        "    padded_list1 = np.array([row + [row[-1]] * (max_length1 - len(row)) for row in best_Y_list])\n",
        "    yy = torch.tensor(padded_list1)\n",
        "\n",
        "    # Save results\n",
        "    np.save('Rosenbrock_cost_CAGES_MFBO.npy',xx)\n",
        "    np.save('Rosenbrock_reward_CAGES_MFBO.npy',yy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkHR4szJ0Gg9",
        "outputId": "ec4b87eb-0a2c-486c-c852-7724165ae075"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current function value =  tensor([[-397.5586]], dtype=torch.float64, grad_fn=<UnsqueezeBackward0>)\n",
            "Current function value =  tensor([[-297.1617]], dtype=torch.float64, grad_fn=<UnsqueezeBackward0>)\n",
            "Current function value =  tensor([[-219.7381]], dtype=torch.float64, grad_fn=<UnsqueezeBackward0>)\n",
            "Current function value =  tensor([[-147.7968]], dtype=torch.float64, grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    }
  ]
}